{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bd4bc834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from DataProcess import data_split_and_merge as da_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271a9be",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "27f30859",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python DataPrepare.py \\\n",
    "    --CenterFlag 1 \\\n",
    "    --mode 'single_file' \\\n",
    "    --input_path './raw_data/1961701.jsonl' \\\n",
    "    --output_folder './tmp' \\\n",
    "    --SampleMode 'short'\n",
    "\n",
    "## with long samples \n",
    "!python DataPrepare.py \\\n",
    "    --CenterFlag 1 \\\n",
    "    --mode 'single_file' \\\n",
    "    --input_path './raw_data/2236632.jsonl' \\\n",
    "    --output_folder './tmp' \\\n",
    "    --SampleMode 'long_3'\n",
    "\n",
    "## with long samples \n",
    "!python DataPrepare.py \\\n",
    "    --CenterFlag 1 \\\n",
    "    --mode 'single_file' \\\n",
    "    --input_path './raw_data/2271377.jsonl' \\\n",
    "    --output_folder './tmp' \\\n",
    "    --SampleMode 'long_3'\n",
    "\n",
    "# new two books for testing\n",
    "!python DataPrepare.py \\\n",
    "    --CenterFlag 1 \\\n",
    "    --mode 'single_file' \\\n",
    "    --input_path './raw_data/2380945.jsonl' \\\n",
    "    --output_folder './tmp' \\\n",
    "    --SampleMode 'long_3'\n",
    "\n",
    "\n",
    "!python DataPrepare.py \\\n",
    "    --CenterFlag 1 \\\n",
    "    --mode 'single_file' \\\n",
    "    --input_path './raw_data/2217876.jsonl' \\\n",
    "    --output_folder './tmp' \\\n",
    "    --SampleMode 'long_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f9e45f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961701.json  2217876.json  2236632.json  2271377.json\t2380945.json\n"
     ]
    }
   ],
   "source": [
    "!ls tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d274bb0",
   "metadata": {},
   "source": [
    "### 统一数据标签风格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2220cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def del_longer_answer(data_path, output_folder):  \n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        f = open(data_path)\n",
    "        data = json.load(f)\n",
    "        text = data['data']\n",
    "        new_text = []\n",
    "        ans_list = []\n",
    "        for i, sample in enumerate(text):\n",
    "            if len(sample['answers']['text'][0]) < 20:\n",
    "                while sample['answers']['text'][0].endswith(' '):\n",
    "                    sample['answers']['text'][0] = sample['answers']['text'][0][0:-1]\n",
    "                if sample['answers']['text'][0].endswith('\\'s'):\n",
    "                    sample['answers']['text'][0] = sample['answers']['text'][0][0:-2]\n",
    "                while sample['answers']['text'][0].startswith(' '): \n",
    "                    sample['answers']['text'][0] = sample['answers']['text'][0][1:]\n",
    "                    sample['answers']['answer_start'][0]+=1\n",
    "                new_text.append(sample)\n",
    "        for idx, sample in enumerate(new_text):\n",
    "                sample['id'] = str(idx)\n",
    "        res_dic = {}\n",
    "        res_dic['data'] =  new_text\n",
    "        print(f\"raw_len:{len(text)}, new_len:{len(new_text)}\")\n",
    "        with open(os.path.join(output_folder, data_path.split('/')[-1].split('.')[-2])+ '.json', 'w', encoding=\"utf-8\") as fp:\n",
    "                json.dump(res_dic, fp) \n",
    "                \n",
    "## 统计答案中以‘s作为结尾的个数 \n",
    "def compute_s_answer(data_path):\n",
    "    f = open(data_path)\n",
    "    data = json.load(f)\n",
    "    text = data['data']\n",
    "\n",
    "    ans_list = []\n",
    "    for i, sample in enumerate(text):\n",
    "        if sample['answers']['text'][0].endswith('\\'s'):\n",
    "            ans_list.append({'index': i, 'text':sample['answers']['text'][0]})\n",
    "\n",
    "    print(data_path.split('/')[-1], len(ans_list), len(ans_list)/len(text)*100)\n",
    "\n",
    "    return ans_list\n",
    "    \n",
    "## 统计答案中前后有空格的个数（这些也可能造成问题\n",
    "def compute_blank_answer(data_path):\n",
    "    f = open(data_path)\n",
    "    data = json.load(f)\n",
    "    text = data['data']\n",
    "\n",
    "    ans_list = []\n",
    "    for i, sample in enumerate(text):\n",
    "        if sample['answers']['text'][0].startswith(' ') or sample['answers']['text'][0].endswith(' '):\n",
    "            ans_list.append({'index': i, 'text':sample['answers']['text'][0]})\n",
    "\n",
    "    print(data_path.split('/')[-1], len(ans_list), len(ans_list)/len(text)*100)\n",
    "\n",
    "    return ans_list\n",
    "    \n",
    "def compute_longer_answer(data_path):\n",
    "    f = open(data_path)\n",
    "    data = json.load(f)\n",
    "    text = data['data']\n",
    "\n",
    "    ans_list = []\n",
    "    for i, sample in enumerate(text):\n",
    "        if len(sample['answers']['text'][0]) >= 20:\n",
    "            ans_list.append({'index': i, 'text':sample['answers']['text'][0]})\n",
    "\n",
    "    print(data_path.split('/')[-1], len(ans_list), len(ans_list)/len(text)*100)\n",
    "\n",
    "    return ans_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e52b0255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_len:354, new_len:326\n",
      "raw_len:408, new_len:406\n",
      "raw_len:168, new_len:168\n",
      "raw_len:117, new_len:117\n",
      "raw_len:278, new_len:278\n"
     ]
    }
   ],
   "source": [
    "del_longer_answer('tmp/2217876.json', 'data')\n",
    "del_longer_answer('tmp/2271377.json', 'data')\n",
    "del_longer_answer('tmp/1961701.json', 'data')\n",
    "del_longer_answer('tmp/2236632.json', 'data')\n",
    "del_longer_answer('tmp/2380945.json', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73835a31",
   "metadata": {},
   "source": [
    "### 训练数据准备: 4 books train, 1 books test (2217876.json etc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8b13cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get train and test file \n",
    "## 4 books train, 1 books test (2380945.json etc) \n",
    "data_1 = \"data/1961701.json\"\n",
    "data_2 = \"data/2380945.json\"\n",
    "data_3 = \"data/2236632.json\"\n",
    "data_4 = \"data/2271377.json\"\n",
    "data_5 = \"data/2217876.json\"\n",
    "\n",
    "data_12 = da_sm.merge_data(data_1, data_2)\n",
    "data_34= da_sm.merge_data(data_3, data_4)\n",
    "data_1234 = da_sm.merge_data(str(data_12), str(data_34)) #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "840bcd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961701+2380945+2236632+2271377.json  2217876.json\t    2271377.json\n",
      "1961701+2380945.json\t\t      2236632+2271377.json  2380945.json\n",
      "1961701.json\t\t\t      2236632.json\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dcc79873",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "prefix='stary-datalab-QAmodel'\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"train/train.json\")\n",
    ").upload_file(\"data/1961701+2380945+2236632+2271377.json\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"test/test.json\")\n",
    ").upload_file(\"data/2217876.json\")\n",
    "\n",
    "training_input_path = f's3://{sess.default_bucket()}/{prefix}/train/train.json'\n",
    "test_input_path = f's3://{sess.default_bucket()}/{prefix}/test/test.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eabf5c7",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4e19d76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-05 13:34:56 Starting - Starting the training job...\n",
      "2022-12-05 13:35:21 Starting - Preparing the instances for trainingProfilerReport-1670247295: InProgress\n",
      ".........\n",
      "2022-12-05 13:36:45 Downloading - Downloading input data\n",
      "2022-12-05 13:36:45 Training - Downloading the training image........................\n",
      "2022-12-05 13:40:56 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-12-05 13:40:59,458 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-12-05 13:40:59,486 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-12-05 13:40:59,493 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-12-05 13:40:59,976 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting datasets>=1.8.0\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.7.1-py3-none-any.whl (451 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 451.7/451.7 kB 8.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.11.0+cu113)\u001b[0m\n",
      "\u001b[34mCollecting evaluate\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.3.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 7.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting transformers\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 63.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0.0,>=0.2.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.4/182.4 kB 34.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 64.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting xxhash\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.9/212.9 kB 37.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (0.70.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (2022.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (0.3.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (1.22.2)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.3.0->-r requirements.txt (line 2)) (4.3.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 82.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\u001b[0m\n",
      "\u001b[34mDownloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 772.3/772.3 kB 51.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.8.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (2.1.1)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 262.1/262.1 kB 31.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 23.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (21.4.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 kB 13.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets>=1.8.0->-r requirements.txt (line 1)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (2022.9.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (1.26.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 1)) (2022.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.8.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, xxhash, regex, multidict, frozenlist, filelock, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, datasets, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.7.1 evaluate-0.3.0 filelock-3.8.1 frozenlist-1.3.3 huggingface-hub-0.11.1 multidict-6.0.3 regex-2022.10.31 responses-0.18.0 tokenizers-0.13.2 transformers-4.25.1 xxhash-3.1.0 yarl-1.8.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2022-12-05 13:41:12,023 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-12-05 13:41:12,023 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-12-05 13:41:12,108 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"do_eval\": true,\n",
      "        \"do_train\": true,\n",
      "        \"doc_stride\": 128,\n",
      "        \"evaluation_strategy\": \"epoch\",\n",
      "        \"greater_is_better\": true,\n",
      "        \"learning_rate\": 3e-05,\n",
      "        \"load_best_model_at_end\": true,\n",
      "        \"max_seq_length\": 512,\n",
      "        \"metric_for_best_model\": \"eval_exact_match\",\n",
      "        \"model_name_or_path\": \"deepset/bert-base-cased-squad2\",\n",
      "        \"num_train_epochs\": 5,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"overwrite_cache\": true,\n",
      "        \"overwrite_output_dir\": true,\n",
      "        \"per_device_train_batch_size\": 12,\n",
      "        \"prediction_loss_only\": false,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"save_total_limit\": 1,\n",
      "        \"train_file\": \"/opt/ml/input/data/train/train.json\",\n",
      "        \"validation_file\": \"/opt/ml/input/data/test/test.json\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"train-QAmodel-stary-4books-eval-on45-2022-12-05-13-34-55-598\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-551641581032/train-QAmodel-stary-4books-eval-on45-2022-12-05-13-34-55-598/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"evaluation_strategy\":\"epoch\",\"greater_is_better\":true,\"learning_rate\":3e-05,\"load_best_model_at_end\":true,\"max_seq_length\":512,\"metric_for_best_model\":\"eval_exact_match\",\"model_name_or_path\":\"deepset/bert-base-cased-squad2\",\"num_train_epochs\":5,\"output_dir\":\"/opt/ml/model\",\"overwrite_cache\":true,\"overwrite_output_dir\":true,\"per_device_train_batch_size\":12,\"prediction_loss_only\":false,\"save_strategy\":\"epoch\",\"save_total_limit\":1,\"train_file\":\"/opt/ml/input/data/train/train.json\",\"validation_file\":\"/opt/ml/input/data/test/test.json\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-551641581032/train-QAmodel-stary-4books-eval-on45-2022-12-05-13-34-55-598/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"evaluation_strategy\":\"epoch\",\"greater_is_better\":true,\"learning_rate\":3e-05,\"load_best_model_at_end\":true,\"max_seq_length\":512,\"metric_for_best_model\":\"eval_exact_match\",\"model_name_or_path\":\"deepset/bert-base-cased-squad2\",\"num_train_epochs\":5,\"output_dir\":\"/opt/ml/model\",\"overwrite_cache\":true,\"overwrite_output_dir\":true,\"per_device_train_batch_size\":12,\"prediction_loss_only\":false,\"save_strategy\":\"epoch\",\"save_total_limit\":1,\"train_file\":\"/opt/ml/input/data/train/train.json\",\"validation_file\":\"/opt/ml/input/data/test/test.json\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"train-QAmodel-stary-4books-eval-on45-2022-12-05-13-34-55-598\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-551641581032/train-QAmodel-stary-4books-eval-on45-2022-12-05-13-34-55-598/source/sourcedir.tar.gz\",\"module_name\":\"run_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--doc_stride\",\"128\",\"--evaluation_strategy\",\"epoch\",\"--greater_is_better\",\"True\",\"--learning_rate\",\"3e-05\",\"--load_best_model_at_end\",\"True\",\"--max_seq_length\",\"512\",\"--metric_for_best_model\",\"eval_exact_match\",\"--model_name_or_path\",\"deepset/bert-base-cased-squad2\",\"--num_train_epochs\",\"5\",\"--output_dir\",\"/opt/ml/model\",\"--overwrite_cache\",\"True\",\"--overwrite_output_dir\",\"True\",\"--per_device_train_batch_size\",\"12\",\"--prediction_loss_only\",\"False\",\"--save_strategy\",\"epoch\",\"--save_total_limit\",\"1\",\"--train_file\",\"/opt/ml/input/data/train/train.json\",\"--validation_file\",\"/opt/ml/input/data/test/test.json\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_DOC_STRIDE=128\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_GREATER_IS_BETTER=true\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=3e-05\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_BEST_MODEL_AT_END=true\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=512\u001b[0m\n",
      "\u001b[34mSM_HP_METRIC_FOR_BEST_MODEL=eval_exact_match\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=deepset/bert-base-cased-squad2\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_OVERWRITE_CACHE=true\u001b[0m\n",
      "\u001b[34mSM_HP_OVERWRITE_OUTPUT_DIR=true\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=12\u001b[0m\n",
      "\u001b[34mSM_HP_PREDICTION_LOSS_ONLY=false\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=1\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/train.json\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FILE=/opt/ml/input/data/test/test.json\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221003-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 run_train.py --do_eval True --do_train True --doc_stride 128 --evaluation_strategy epoch --greater_is_better True --learning_rate 3e-05 --load_best_model_at_end True --max_seq_length 512 --metric_for_best_model eval_exact_match --model_name_or_path deepset/bert-base-cased-squad2 --num_train_epochs 5 --output_dir /opt/ml/model --overwrite_cache True --overwrite_output_dir True --per_device_train_batch_size 12 --prediction_loss_only False --save_strategy epoch --save_total_limit 1 --train_file /opt/ml/input/data/train/train.json --validation_file /opt/ml/input/data/test/test.json\u001b[0m\n",
      "\u001b[34m[2022-12-05 13:41:15.387: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.25.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=True,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=3e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=True,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=passive,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/runs/Dec05_13-41-19_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=eval_exact_match,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=5.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=12,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=epoch,\u001b[0m\n",
      "\u001b[34msave_total_limit=1,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - WARNING - datasets.builder - Using custom data configuration default-ee9820a933790e63\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.8/site-packages/datasets/packaged_modules/json\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-ee9820a933790e63/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-ee9820a933790e63/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 7281.78it/s]\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - INFO - datasets.download.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - INFO - datasets.utils.py_utils - Spawning 2 processes for 2 objects in slices of [1, 1]\u001b[0m\n",
      "\u001b[34mExtracting data files #1:   0%|          | 0/1 [00:00<?, ?obj/s]#033[A\u001b[0m\n",
      "\u001b[34mExtracting data files #0:   0%|          | 0/1 [00:00<?, ?obj/s]\u001b[0m\n",
      "\u001b[34mExtracting data files #0: 100%|██████████| 1/1 [00:00<00:00, 263.66obj/s]\u001b[0m\n",
      "\u001b[34mExtracting data files #1: 100%|██████████| 1/1 [00:00<00:00, 227.38obj/s]\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - INFO - datasets.utils.py_utils - Finished 2 processes\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - INFO - datasets.utils.py_utils - Unpacked 2 objects\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - INFO - datasets.utils.info_utils - Unable to verify checksums.\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - INFO - datasets.builder - Generating train split\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - INFO - datasets.builder - Generating validation split\u001b[0m\n",
      "\u001b[34mGenerating validation split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:19 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-ee9820a933790e63/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 306.66it/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/508 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 508/508 [00:00<00:00, 706kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:654] 2022-12-05 13:41:19,890 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:654] 2022-12-05 13:41:19,890 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:706] 2022-12-05 13:41:19,894 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/bert-base-cased-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"name\": \"Bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:706] 2022-12-05 13:41:19,894 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/bert-base-cased-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"name\": \"Bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/152 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 152/152 [00:00<00:00, 212kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:654] 2022-12-05 13:41:19,940 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:654] 2022-12-05 13:41:19,940 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:706] 2022-12-05 13:41:19,941 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/bert-base-cased-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"name\": \"Bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:706] 2022-12-05 13:41:19,941 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/bert-base-cased-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"name\": \"Bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/213k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 213k/213k [00:00<00:00, 45.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/112 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 112/112 [00:00<00:00, 165kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1799] 2022-12-05 13:41:20,090 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/vocab.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1799] 2022-12-05 13:41:20,090 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/vocab.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1799] 2022-12-05 13:41:20,090 >> loading file tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1799] 2022-12-05 13:41:20,090 >> loading file tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1799] 2022-12-05 13:41:20,090 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1799] 2022-12-05 13:41:20,090 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1799] 2022-12-05 13:41:20,090 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1799] 2022-12-05 13:41:20,090 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1799] 2022-12-05 13:41:20,090 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1799] 2022-12-05 13:41:20,090 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:654] 2022-12-05 13:41:20,090 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:654] 2022-12-05 13:41:20,090 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:706] 2022-12-05 13:41:20,091 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/bert-base-cased-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"name\": \"Bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:706] 2022-12-05 13:41:20,091 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/bert-base-cased-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"name\": \"Bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:654] 2022-12-05 13:41:20,121 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:654] 2022-12-05 13:41:20,121 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:706] 2022-12-05 13:41:20,122 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/bert-base-cased-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"name\": \"Bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:706] 2022-12-05 13:41:20,122 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/bert-base-cased-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"name\": \"Bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/433M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 5.26M/433M [00:00<00:08, 52.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 11.1M/433M [00:00<00:07, 55.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 16.9M/433M [00:00<00:07, 57.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 22.7M/433M [00:00<00:07, 57.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 28.5M/433M [00:00<00:07, 57.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 34.4M/433M [00:00<00:06, 58.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 40.4M/433M [00:00<00:06, 58.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 46.3M/433M [00:00<00:06, 58.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 52.2M/433M [00:00<00:06, 59.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 58.2M/433M [00:01<00:06, 59.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▍        | 64.1M/433M [00:01<00:06, 59.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 70.1M/433M [00:01<00:06, 59.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 76.1M/433M [00:01<00:06, 59.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 82.0M/433M [00:01<00:05, 59.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|██        | 88.0M/433M [00:01<00:05, 59.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 93.9M/433M [00:01<00:05, 59.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 99.9M/433M [00:01<00:05, 59.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 106M/433M [00:01<00:05, 60.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 112M/433M [00:01<00:05, 60.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 119M/433M [00:02<00:05, 61.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 125M/433M [00:02<00:04, 62.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|███       | 131M/433M [00:02<00:04, 62.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 137M/433M [00:02<00:04, 62.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 144M/433M [00:02<00:04, 62.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▍      | 150M/433M [00:02<00:04, 63.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 157M/433M [00:02<00:04, 63.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 163M/433M [00:02<00:04, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 169M/433M [00:02<00:04, 63.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 176M/433M [00:02<00:04, 63.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 182M/433M [00:03<00:03, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 188M/433M [00:03<00:03, 63.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 195M/433M [00:03<00:03, 62.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▋     | 201M/433M [00:03<00:03, 63.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 207M/433M [00:03<00:03, 63.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 214M/433M [00:03<00:03, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 220M/433M [00:03<00:03, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 226M/433M [00:03<00:03, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▎    | 233M/433M [00:03<00:03, 63.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 239M/433M [00:03<00:03, 63.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 245M/433M [00:04<00:02, 63.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 252M/433M [00:04<00:02, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|█████▉    | 258M/433M [00:04<00:02, 63.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 264M/433M [00:04<00:02, 63.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 271M/433M [00:04<00:02, 62.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 277M/433M [00:04<00:02, 62.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▌   | 283M/433M [00:04<00:02, 62.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 289M/433M [00:04<00:02, 62.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 296M/433M [00:04<00:02, 62.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 302M/433M [00:04<00:02, 62.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 308M/433M [00:05<00:01, 63.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 315M/433M [00:05<00:01, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 321M/433M [00:05<00:01, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 327M/433M [00:05<00:01, 62.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 334M/433M [00:05<00:01, 62.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 340M/433M [00:05<00:01, 61.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|███████▉  | 346M/433M [00:05<00:01, 62.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████▏ | 353M/433M [00:05<00:01, 62.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 359M/433M [00:05<00:01, 62.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 365M/433M [00:05<00:01, 62.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 372M/433M [00:06<00:00, 63.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 378M/433M [00:06<00:00, 63.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▊ | 384M/433M [00:06<00:00, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|█████████ | 391M/433M [00:06<00:00, 63.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 397M/433M [00:06<00:00, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 403M/433M [00:06<00:00, 61.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 410M/433M [00:06<00:00, 62.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 416M/433M [00:06<00:00, 60.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 422M/433M [00:06<00:00, 60.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 429M/433M [00:06<00:00, 62.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 433M/433M [00:07<00:00, 61.8MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2204] 2022-12-05 13:41:27,214 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2204] 2022-12-05 13:41:27,214 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--deepset--bert-base-cased-squad2/snapshots/4253cf63de501bbee34f59520ccf68bb40538972/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2708] 2022-12-05 13:41:28,670 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2716] 2022-12-05 13:41:28,671 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at deepset/bert-base-cased-squad2.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2708] 2022-12-05 13:41:28,670 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2716] 2022-12-05 13:41:28,671 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at deepset/bert-base-cased-squad2.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-ee9820a933790e63/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-70145eb554e2928d.arrow\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 1/1 [00:00<00:00,  1.80ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 1/1 [00:00<00:00,  1.80ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m12/05/2022 13:41:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-ee9820a933790e63/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-eaabfd7d707e41a8.arrow\u001b[0m\n",
      "\u001b[34mRunning tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.13ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.12ba/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 4.53k/4.53k [00:00<00:00, 5.39MB/s]\u001b[0m\n",
      "\u001b[34mDownloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading extra modules: 100%|██████████| 3.32k/3.32k [00:00<00:00, 4.29MB/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1634] 2022-12-05 13:41:35,185 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1634] 2022-12-05 13:41:35,185 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1635] 2022-12-05 13:41:35,185 >>   Num examples = 972\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1636] 2022-12-05 13:41:35,186 >>   Num Epochs = 5\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1637] 2022-12-05 13:41:35,186 >>   Instantaneous batch size per device = 12\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1635] 2022-12-05 13:41:35,185 >>   Num examples = 972\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1636] 2022-12-05 13:41:35,186 >>   Num Epochs = 5\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1637] 2022-12-05 13:41:35,186 >>   Instantaneous batch size per device = 12\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1638] 2022-12-05 13:41:35,186 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1639] 2022-12-05 13:41:35,186 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1640] 2022-12-05 13:41:35,186 >>   Total optimization steps = 405\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1638] 2022-12-05 13:41:35,186 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1639] 2022-12-05 13:41:35,186 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1640] 2022-12-05 13:41:35,186 >>   Total optimization steps = 405\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1641] 2022-12-05 13:41:35,187 >>   Number of trainable parameters = 107721218\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1641] 2022-12-05 13:41:35,187 >>   Number of trainable parameters = 107721218\u001b[0m\n",
      "\u001b[34m0%|          | 0/405 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-12-05 13:41:35.301: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.25.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2022-12-05 13:41:35.358 algo-1:58 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-12-05 13:41:35.569 algo-1:58 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-12-05 13:41:35.571 algo-1:58 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-12-05 13:41:35.571 algo-1:58 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-12-05 13:41:35.572 algo-1:58 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-12-05 13:41:35.572 algo-1:58 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m0%|          | 1/405 [00:01<06:56,  1.03s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/405 [00:01<04:21,  1.54it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 3/405 [00:01<03:32,  1.89it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4/405 [00:02<03:08,  2.13it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 5/405 [00:02<02:54,  2.30it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 6/405 [00:02<02:46,  2.40it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/405 [00:03<02:41,  2.46it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/405 [00:03<02:38,  2.51it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 9/405 [00:04<02:35,  2.54it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 10/405 [00:04<02:33,  2.57it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/405 [00:04<02:32,  2.59it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12/405 [00:05<02:31,  2.60it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 13/405 [00:05<02:30,  2.61it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 14/405 [00:05<02:29,  2.61it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 15/405 [00:06<02:29,  2.62it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 16/405 [00:06<02:28,  2.63it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 17/405 [00:07<02:27,  2.63it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 18/405 [00:07<02:27,  2.62it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 19/405 [00:07<02:27,  2.62it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 20/405 [00:08<02:26,  2.62it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 21/405 [00:08<02:26,  2.63it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 22/405 [00:09<02:25,  2.63it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 23/405 [00:09<02:25,  2.63it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 24/405 [00:09<02:24,  2.63it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 25/405 [00:10<02:24,  2.63it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 26/405 [00:10<02:24,  2.63it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 27/405 [00:10<02:24,  2.62it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 28/405 [00:11<02:24,  2.62it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 29/405 [00:11<02:23,  2.62it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 30/405 [00:12<02:22,  2.62it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 31/405 [00:12<02:22,  2.62it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 32/405 [00:12<02:22,  2.62it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 33/405 [00:13<02:21,  2.62it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 34/405 [00:13<02:21,  2.62it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 35/405 [00:13<02:21,  2.62it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 36/405 [00:14<02:20,  2.62it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 37/405 [00:14<02:20,  2.63it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 38/405 [00:15<02:19,  2.62it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 39/405 [00:15<02:19,  2.62it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 40/405 [00:15<02:19,  2.62it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 41/405 [00:16<02:18,  2.62it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 42/405 [00:16<02:18,  2.63it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 43/405 [00:17<02:17,  2.63it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 44/405 [00:17<02:16,  2.64it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 45/405 [00:17<02:16,  2.64it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 46/405 [00:18<02:16,  2.63it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 47/405 [00:18<02:16,  2.63it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 48/405 [00:18<02:15,  2.63it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 49/405 [00:19<02:16,  2.61it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 50/405 [00:19<02:16,  2.61it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 51/405 [00:20<02:16,  2.60it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 52/405 [00:20<02:16,  2.59it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 53/405 [00:20<02:15,  2.60it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 54/405 [00:21<02:14,  2.61it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 55/405 [00:21<02:14,  2.60it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 56/405 [00:22<02:13,  2.60it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 57/405 [00:22<02:13,  2.62it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 58/405 [00:22<02:12,  2.62it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 59/405 [00:23<02:11,  2.62it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 60/405 [00:23<02:11,  2.62it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 61/405 [00:23<02:11,  2.62it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 62/405 [00:24<02:10,  2.62it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 63/405 [00:24<02:10,  2.63it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 64/405 [00:25<02:10,  2.62it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 65/405 [00:25<02:10,  2.61it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 66/405 [00:25<02:09,  2.62it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 67/405 [00:26<02:11,  2.57it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 68/405 [00:26<02:11,  2.57it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 69/405 [00:27<02:10,  2.58it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 70/405 [00:27<02:10,  2.57it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 71/405 [00:27<02:09,  2.57it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 72/405 [00:28<02:08,  2.58it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 73/405 [00:28<02:09,  2.56it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 74/405 [00:28<02:08,  2.57it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 75/405 [00:29<02:09,  2.56it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 76/405 [00:29<02:07,  2.57it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 77/405 [00:30<02:07,  2.58it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 78/405 [00:30<02:07,  2.57it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 79/405 [00:30<02:06,  2.58it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 80/405 [00:31<02:06,  2.58it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 81/405 [00:31<02:05,  2.58it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:703] 2022-12-05 13:42:06,858 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:703] 2022-12-05 13:42:06,858 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2944] 2022-12-05 13:42:06,861 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2944] 2022-12-05 13:42:06,861 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2946] 2022-12-05 13:42:06,861 >>   Num examples = 326\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2949] 2022-12-05 13:42:06,861 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2946] 2022-12-05 13:42:06,861 >>   Num examples = 326\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2949] 2022-12-05 13:42:06,861 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/41 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 3/41 [00:00<00:02, 17.85it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 5/41 [00:00<00:02, 14.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 7/41 [00:00<00:02, 13.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 9/41 [00:00<00:02, 12.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 11/41 [00:00<00:02, 12.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 13/41 [00:01<00:02, 12.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 15/41 [00:01<00:02, 12.12it/s]#033[A\u001b[0m\n",
      "\u001b[34m41%|████▏     | 17/41 [00:01<00:01, 12.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▋     | 19/41 [00:01<00:01, 11.97it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 21/41 [00:01<00:01, 11.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 23/41 [00:01<00:01, 11.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 25/41 [00:02<00:01, 11.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 27/41 [00:02<00:01, 11.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 29/41 [00:02<00:01, 11.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 31/41 [00:02<00:00, 11.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 33/41 [00:02<00:00, 11.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 35/41 [00:02<00:00, 11.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 37/41 [00:03<00:00, 11.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 39/41 [00:03<00:00, 11.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 41/41 [00:03<00:00, 12.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m12/05/2022 13:42:10 - INFO - utils_qa - Post-processing 326 example predictions split into 326 features.\u001b[0m\n",
      "\u001b[34m0%|          | 0/326 [00:00<?, ?it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m9%|▊         | 28/326 [00:00<00:01, 272.78it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 56/326 [00:00<00:01, 251.24it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 84/326 [00:00<00:00, 260.40it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 112/326 [00:00<00:00, 265.35it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 140/326 [00:00<00:00, 267.90it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 167/326 [00:00<00:00, 257.21it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 193/326 [00:00<00:00, 253.61it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 221/326 [00:00<00:00, 260.53it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 249/326 [00:00<00:00, 265.27it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 277/326 [00:01<00:00, 268.67it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 305/326 [00:01<00:00, 270.63it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 326/326 [00:01<00:00, 264.99it/s]\u001b[0m\n",
      "\u001b[34m12/05/2022 13:42:11 - INFO - utils_qa - Saving predictions to /opt/ml/model/eval_predictions.json.\u001b[0m\n",
      "\u001b[34m12/05/2022 13:42:11 - INFO - utils_qa - Saving nbest_preds to /opt/ml/model/eval_nbest_predictions.json.\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_exact_match': 93.25153374233129, 'eval_f1': 94.06952965235173, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m20%|██        | 81/405 [00:36<02:05,  2.58it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 41/41 [00:05<00:00, 12.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2693] 2022-12-05 13:42:12,110 >> Saving model checkpoint to /opt/ml/model/checkpoint-81\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2693] 2022-12-05 13:42:12,110 >> Saving model checkpoint to /opt/ml/model/checkpoint-81\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:447] 2022-12-05 13:42:12,111 >> Configuration saved in /opt/ml/model/checkpoint-81/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:447] 2022-12-05 13:42:12,111 >> Configuration saved in /opt/ml/model/checkpoint-81/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1637] 2022-12-05 13:42:12,886 >> Model weights saved in /opt/ml/model/checkpoint-81/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1637] 2022-12-05 13:42:12,886 >> Model weights saved in /opt/ml/model/checkpoint-81/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2157] 2022-12-05 13:42:12,887 >> tokenizer config file saved in /opt/ml/model/checkpoint-81/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2157] 2022-12-05 13:42:12,887 >> tokenizer config file saved in /opt/ml/model/checkpoint-81/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2164] 2022-12-05 13:42:12,887 >> Special tokens file saved in /opt/ml/model/checkpoint-81/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2164] 2022-12-05 13:42:12,887 >> Special tokens file saved in /opt/ml/model/checkpoint-81/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m20%|██        | 82/405 [00:39<14:19,  2.66s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 83/405 [00:40<10:36,  1.98s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 84/405 [00:40<08:01,  1.50s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 85/405 [00:40<06:13,  1.17s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 86/405 [00:41<04:57,  1.07it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 87/405 [00:41<04:03,  1.30it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 88/405 [00:41<03:26,  1.54it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 89/405 [00:42<03:00,  1.75it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 90/405 [00:42<02:42,  1.94it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 91/405 [00:43<02:28,  2.11it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 92/405 [00:43<02:19,  2.24it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 93/405 [00:43<02:13,  2.34it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 94/405 [00:44<02:11,  2.37it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 95/405 [00:44<02:07,  2.42it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 96/405 [00:45<02:05,  2.47it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 97/405 [00:45<02:03,  2.50it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 98/405 [00:45<02:01,  2.53it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 99/405 [00:46<02:00,  2.54it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 100/405 [00:46<01:59,  2.56it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 101/405 [00:46<01:58,  2.56it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 102/405 [00:47<01:58,  2.55it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 103/405 [00:47<01:57,  2.56it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 104/405 [00:48<01:57,  2.57it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 105/405 [00:48<01:56,  2.57it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 106/405 [00:48<01:56,  2.57it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 107/405 [00:49<01:55,  2.58it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 108/405 [00:49<01:55,  2.58it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 109/405 [00:50<01:54,  2.58it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 110/405 [00:50<01:54,  2.58it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 111/405 [00:50<01:54,  2.57it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 112/405 [00:51<01:54,  2.57it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 113/405 [00:51<01:53,  2.57it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 114/405 [00:52<01:52,  2.58it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 115/405 [00:52<01:52,  2.58it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 116/405 [00:52<01:51,  2.59it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 117/405 [00:53<01:50,  2.60it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 118/405 [00:53<01:50,  2.59it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 119/405 [00:53<01:50,  2.60it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 120/405 [00:54<01:49,  2.61it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 121/405 [00:54<01:48,  2.61it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 122/405 [00:55<01:48,  2.61it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 123/405 [00:55<01:48,  2.61it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 124/405 [00:55<01:47,  2.60it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 125/405 [00:56<01:47,  2.61it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 126/405 [00:56<01:47,  2.59it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 127/405 [00:57<01:47,  2.60it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 128/405 [00:57<01:46,  2.59it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 129/405 [00:57<01:46,  2.58it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 130/405 [00:58<01:46,  2.58it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 131/405 [00:58<01:46,  2.58it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 132/405 [00:58<01:45,  2.59it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 133/405 [00:59<01:44,  2.60it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 134/405 [00:59<01:44,  2.59it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 135/405 [01:00<01:44,  2.60it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 136/405 [01:00<01:43,  2.60it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 137/405 [01:00<01:43,  2.60it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 138/405 [01:01<01:42,  2.60it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 139/405 [01:01<01:42,  2.60it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 140/405 [01:02<01:41,  2.60it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 141/405 [01:02<01:41,  2.60it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 142/405 [01:02<01:41,  2.60it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 143/405 [01:03<01:40,  2.60it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 144/405 [01:03<01:40,  2.60it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 145/405 [01:03<01:39,  2.60it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 146/405 [01:04<01:39,  2.60it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 147/405 [01:04<01:39,  2.60it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 148/405 [01:05<01:39,  2.59it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 149/405 [01:05<01:38,  2.59it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 150/405 [01:05<01:38,  2.58it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 151/405 [01:06<01:38,  2.59it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 152/405 [01:06<01:37,  2.59it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 153/405 [01:07<01:37,  2.58it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 154/405 [01:07<01:37,  2.58it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 155/405 [01:07<01:36,  2.59it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 156/405 [01:08<01:36,  2.59it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 157/405 [01:08<01:35,  2.59it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 158/405 [01:08<01:35,  2.59it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 159/405 [01:09<01:34,  2.59it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 160/405 [01:09<01:34,  2.59it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 161/405 [01:10<01:34,  2.59it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 162/405 [01:10<01:33,  2.59it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:703] 2022-12-05 13:42:45,711 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:703] 2022-12-05 13:42:45,711 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2944] 2022-12-05 13:42:45,714 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2944] 2022-12-05 13:42:45,714 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2946] 2022-12-05 13:42:45,714 >>   Num examples = 326\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2949] 2022-12-05 13:42:45,714 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2946] 2022-12-05 13:42:45,714 >>   Num examples = 326\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2949] 2022-12-05 13:42:45,714 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/41 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 3/41 [00:00<00:02, 17.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 5/41 [00:00<00:02, 14.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 7/41 [00:00<00:02, 13.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 9/41 [00:00<00:02, 12.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 11/41 [00:00<00:02, 12.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 13/41 [00:01<00:02, 12.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 15/41 [00:01<00:02, 12.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m41%|████▏     | 17/41 [00:01<00:02, 11.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▋     | 19/41 [00:01<00:01, 11.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 21/41 [00:01<00:01, 11.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 23/41 [00:01<00:01, 11.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 25/41 [00:02<00:01, 11.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 27/41 [00:02<00:01, 11.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 29/41 [00:02<00:01, 11.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 31/41 [00:02<00:00, 11.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 33/41 [00:02<00:00, 11.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 35/41 [00:02<00:00, 11.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 37/41 [00:03<00:00, 11.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 39/41 [00:03<00:00, 11.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 41/41 [00:03<00:00, 12.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m12/05/2022 13:42:49 - INFO - utils_qa - Post-processing 326 example predictions split into 326 features.\u001b[0m\n",
      "\u001b[34m0%|          | 0/326 [00:00<?, ?it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m9%|▊         | 28/326 [00:00<00:01, 271.88it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 56/326 [00:00<00:01, 250.36it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 84/326 [00:00<00:00, 261.79it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 112/326 [00:00<00:00, 265.46it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 140/326 [00:00<00:00, 267.38it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 168/326 [00:00<00:00, 269.52it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 196/326 [00:00<00:00, 271.22it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 224/326 [00:00<00:00, 260.28it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 252/326 [00:00<00:00, 264.95it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 279/326 [00:01<00:00, 256.74it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 305/326 [00:01<00:00, 250.70it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 326/326 [00:01<00:00, 260.63it/s]\u001b[0m\n",
      "\u001b[34m12/05/2022 13:42:50 - INFO - utils_qa - Saving predictions to /opt/ml/model/eval_predictions.json.\u001b[0m\n",
      "\u001b[34m12/05/2022 13:42:50 - INFO - utils_qa - Saving nbest_preds to /opt/ml/model/eval_nbest_predictions.json.\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_exact_match': 93.25153374233129, 'eval_f1': 94.09741587655698, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m40%|████      | 162/405 [01:15<01:33,  2.59it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 41/41 [00:05<00:00, 12.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2693] 2022-12-05 13:42:51,030 >> Saving model checkpoint to /opt/ml/model/checkpoint-162\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2693] 2022-12-05 13:42:51,030 >> Saving model checkpoint to /opt/ml/model/checkpoint-162\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:447] 2022-12-05 13:42:51,031 >> Configuration saved in /opt/ml/model/checkpoint-162/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:447] 2022-12-05 13:42:51,031 >> Configuration saved in /opt/ml/model/checkpoint-162/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1637] 2022-12-05 13:42:51,805 >> Model weights saved in /opt/ml/model/checkpoint-162/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1637] 2022-12-05 13:42:51,805 >> Model weights saved in /opt/ml/model/checkpoint-162/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2157] 2022-12-05 13:42:51,806 >> tokenizer config file saved in /opt/ml/model/checkpoint-162/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2157] 2022-12-05 13:42:51,806 >> tokenizer config file saved in /opt/ml/model/checkpoint-162/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2164] 2022-12-05 13:42:51,806 >> Special tokens file saved in /opt/ml/model/checkpoint-162/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2164] 2022-12-05 13:42:51,806 >> Special tokens file saved in /opt/ml/model/checkpoint-162/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m40%|████      | 163/405 [01:18<10:48,  2.68s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 164/405 [01:18<07:59,  1.99s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 165/405 [01:19<06:02,  1.51s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 166/405 [01:19<04:40,  1.17s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 167/405 [01:20<03:42,  1.07it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 168/405 [01:20<03:02,  1.30it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 169/405 [01:20<02:34,  1.53it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 170/405 [01:21<02:15,  1.74it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 171/405 [01:21<02:00,  1.94it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 172/405 [01:22<01:51,  2.10it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 173/405 [01:22<01:44,  2.22it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 174/405 [01:22<01:39,  2.33it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 175/405 [01:23<01:36,  2.39it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 176/405 [01:23<01:33,  2.45it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 177/405 [01:23<01:31,  2.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 178/405 [01:24<01:30,  2.51it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 179/405 [01:24<01:29,  2.53it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 180/405 [01:25<01:28,  2.54it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 181/405 [01:25<01:27,  2.56it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 182/405 [01:25<01:26,  2.57it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 183/405 [01:26<01:26,  2.58it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 184/405 [01:26<01:26,  2.56it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 185/405 [01:27<01:25,  2.56it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 186/405 [01:27<01:25,  2.57it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 187/405 [01:27<01:25,  2.56it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 188/405 [01:28<01:24,  2.56it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 189/405 [01:28<01:24,  2.57it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 190/405 [01:29<01:23,  2.56it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 191/405 [01:29<01:23,  2.55it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 192/405 [01:29<01:23,  2.56it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 193/405 [01:30<01:22,  2.58it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 194/405 [01:30<01:21,  2.58it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 195/405 [01:30<01:21,  2.59it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 196/405 [01:31<01:20,  2.59it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 197/405 [01:31<01:20,  2.60it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 198/405 [01:32<01:19,  2.61it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 199/405 [01:32<01:18,  2.61it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 200/405 [01:32<01:18,  2.61it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 201/405 [01:33<01:18,  2.60it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 202/405 [01:33<01:18,  2.60it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 203/405 [01:34<01:17,  2.59it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 204/405 [01:34<01:17,  2.59it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 205/405 [01:34<01:17,  2.60it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 206/405 [01:35<01:16,  2.60it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 207/405 [01:35<01:16,  2.59it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 208/405 [01:35<01:15,  2.59it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 209/405 [01:36<01:15,  2.59it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 210/405 [01:36<01:15,  2.59it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 211/405 [01:37<01:15,  2.58it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 212/405 [01:37<01:14,  2.58it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 213/405 [01:37<01:14,  2.58it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 214/405 [01:38<01:14,  2.57it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 215/405 [01:38<01:13,  2.57it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 216/405 [01:39<01:13,  2.57it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 217/405 [01:39<01:13,  2.56it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 218/405 [01:39<01:12,  2.57it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 219/405 [01:40<01:12,  2.57it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 220/405 [01:40<01:12,  2.56it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 221/405 [01:40<01:11,  2.56it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 222/405 [01:41<01:11,  2.57it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 223/405 [01:41<01:11,  2.56it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 224/405 [01:42<01:10,  2.56it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 225/405 [01:42<01:09,  2.57it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 226/405 [01:42<01:09,  2.58it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 227/405 [01:43<01:09,  2.57it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 228/405 [01:43<01:08,  2.57it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 229/405 [01:44<01:08,  2.57it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 230/405 [01:44<01:08,  2.57it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 231/405 [01:44<01:07,  2.58it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 232/405 [01:45<01:06,  2.59it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 233/405 [01:45<01:06,  2.59it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 234/405 [01:46<01:06,  2.59it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 235/405 [01:46<01:05,  2.58it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 236/405 [01:46<01:05,  2.58it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 237/405 [01:47<01:04,  2.59it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 238/405 [01:47<01:04,  2.58it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 239/405 [01:47<01:04,  2.59it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 240/405 [01:48<01:03,  2.58it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 241/405 [01:48<01:03,  2.58it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 242/405 [01:49<01:02,  2.59it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 243/405 [01:49<01:02,  2.59it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:703] 2022-12-05 13:43:24,707 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:703] 2022-12-05 13:43:24,707 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2944] 2022-12-05 13:43:24,709 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2944] 2022-12-05 13:43:24,709 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2946] 2022-12-05 13:43:24,709 >>   Num examples = 326\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2949] 2022-12-05 13:43:24,709 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2946] 2022-12-05 13:43:24,709 >>   Num examples = 326\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2949] 2022-12-05 13:43:24,709 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/41 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 3/41 [00:00<00:02, 17.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 5/41 [00:00<00:02, 14.31it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 7/41 [00:00<00:02, 13.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 9/41 [00:00<00:02, 12.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 11/41 [00:00<00:02, 12.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 13/41 [00:01<00:02, 12.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 15/41 [00:01<00:02, 12.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m41%|████▏     | 17/41 [00:01<00:01, 12.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▋     | 19/41 [00:01<00:01, 11.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 21/41 [00:01<00:01, 11.90it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 23/41 [00:01<00:01, 11.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 25/41 [00:02<00:01, 11.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 27/41 [00:02<00:01, 11.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 29/41 [00:02<00:01, 11.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 31/41 [00:02<00:00, 11.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 33/41 [00:02<00:00, 11.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 35/41 [00:02<00:00, 11.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 37/41 [00:03<00:00, 11.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 39/41 [00:03<00:00, 11.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 41/41 [00:03<00:00, 12.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m12/05/2022 13:43:28 - INFO - utils_qa - Post-processing 326 example predictions split into 326 features.\u001b[0m\n",
      "\u001b[34m0%|          | 0/326 [00:00<?, ?it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m9%|▊         | 28/326 [00:00<00:01, 278.46it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 56/326 [00:00<00:01, 255.22it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 85/326 [00:00<00:00, 266.65it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 112/326 [00:00<00:00, 264.20it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 141/326 [00:00<00:00, 270.12it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 169/326 [00:00<00:00, 273.26it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 198/326 [00:00<00:00, 275.72it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 226/326 [00:00<00:00, 272.86it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 255/326 [00:00<00:00, 275.59it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 283/326 [00:01<00:00, 265.21it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 311/326 [00:01<00:00, 266.71it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 326/326 [00:01<00:00, 269.27it/s]\u001b[0m\n",
      "\u001b[34m12/05/2022 13:43:29 - INFO - utils_qa - Saving predictions to /opt/ml/model/eval_predictions.json.\u001b[0m\n",
      "\u001b[34m12/05/2022 13:43:29 - INFO - utils_qa - Saving nbest_preds to /opt/ml/model/eval_nbest_predictions.json.\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_exact_match': 92.94478527607362, 'eval_f1': 93.95705521472391, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m60%|██████    | 243/405 [01:54<01:02,  2.59it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 41/41 [00:05<00:00, 12.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2693] 2022-12-05 13:43:30,055 >> Saving model checkpoint to /opt/ml/model/checkpoint-243\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2693] 2022-12-05 13:43:30,055 >> Saving model checkpoint to /opt/ml/model/checkpoint-243\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:447] 2022-12-05 13:43:30,056 >> Configuration saved in /opt/ml/model/checkpoint-243/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:447] 2022-12-05 13:43:30,056 >> Configuration saved in /opt/ml/model/checkpoint-243/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1637] 2022-12-05 13:43:30,776 >> Model weights saved in /opt/ml/model/checkpoint-243/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1637] 2022-12-05 13:43:30,776 >> Model weights saved in /opt/ml/model/checkpoint-243/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2157] 2022-12-05 13:43:30,777 >> tokenizer config file saved in /opt/ml/model/checkpoint-243/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2157] 2022-12-05 13:43:30,777 >> tokenizer config file saved in /opt/ml/model/checkpoint-243/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2164] 2022-12-05 13:43:30,777 >> Special tokens file saved in /opt/ml/model/checkpoint-243/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2164] 2022-12-05 13:43:30,777 >> Special tokens file saved in /opt/ml/model/checkpoint-243/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2771] 2022-12-05 13:43:32,292 >> Deleting older checkpoint [/opt/ml/model/checkpoint-162] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2771] 2022-12-05 13:43:32,292 >> Deleting older checkpoint [/opt/ml/model/checkpoint-162] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m60%|██████    | 244/405 [01:57<07:15,  2.71s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 245/405 [01:58<05:21,  2.01s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 246/405 [01:58<04:02,  1.52s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 247/405 [01:58<03:06,  1.18s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 248/405 [01:59<02:27,  1.06it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 249/405 [01:59<02:00,  1.29it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 250/405 [01:59<01:41,  1.52it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 251/405 [02:00<01:28,  1.74it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 252/405 [02:00<01:19,  1.93it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 253/405 [02:01<01:12,  2.09it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 254/405 [02:01<01:08,  2.21it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 255/405 [02:01<01:04,  2.32it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 256/405 [02:02<01:02,  2.40it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 257/405 [02:02<01:00,  2.46it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 258/405 [02:03<00:59,  2.48it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 259/405 [02:03<00:58,  2.50it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 260/405 [02:03<00:57,  2.52it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 261/405 [02:04<00:56,  2.54it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 262/405 [02:04<00:56,  2.55it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 263/405 [02:04<00:55,  2.56it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 264/405 [02:05<00:55,  2.56it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 265/405 [02:05<00:54,  2.56it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 266/405 [02:06<00:54,  2.57it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 267/405 [02:06<00:53,  2.57it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 268/405 [02:06<00:53,  2.58it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 269/405 [02:07<00:52,  2.59it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 270/405 [02:07<00:52,  2.58it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 271/405 [02:08<00:51,  2.58it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 272/405 [02:08<00:51,  2.59it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 273/405 [02:08<00:50,  2.59it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 274/405 [02:09<00:50,  2.58it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 275/405 [02:09<00:50,  2.59it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 276/405 [02:10<00:49,  2.59it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 277/405 [02:10<00:49,  2.59it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 278/405 [02:10<00:48,  2.59it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 279/405 [02:11<00:48,  2.60it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 280/405 [02:11<00:48,  2.60it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 281/405 [02:11<00:47,  2.60it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 282/405 [02:12<00:47,  2.59it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 283/405 [02:12<00:47,  2.59it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 284/405 [02:13<00:46,  2.59it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 285/405 [02:13<00:46,  2.59it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 286/405 [02:13<00:46,  2.58it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 287/405 [02:14<00:45,  2.58it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 288/405 [02:14<00:45,  2.58it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 289/405 [02:15<00:45,  2.58it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 290/405 [02:15<00:44,  2.58it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 291/405 [02:15<00:44,  2.58it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 292/405 [02:16<00:43,  2.59it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 293/405 [02:16<00:43,  2.59it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 294/405 [02:16<00:42,  2.58it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 295/405 [02:17<00:42,  2.58it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 296/405 [02:17<00:42,  2.58it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 297/405 [02:18<00:41,  2.57it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 298/405 [02:18<00:41,  2.56it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 299/405 [02:18<00:41,  2.58it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 300/405 [02:19<00:40,  2.57it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 301/405 [02:19<00:40,  2.56it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 302/405 [02:20<00:40,  2.57it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 303/405 [02:20<00:39,  2.57it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 304/405 [02:20<00:39,  2.58it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 305/405 [02:21<00:38,  2.58it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 306/405 [02:21<00:38,  2.58it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 307/405 [02:22<00:37,  2.58it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 308/405 [02:22<00:37,  2.59it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 309/405 [02:22<00:37,  2.59it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 310/405 [02:23<00:36,  2.58it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 311/405 [02:23<00:36,  2.58it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 312/405 [02:23<00:36,  2.58it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 313/405 [02:24<00:35,  2.58it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 314/405 [02:24<00:35,  2.58it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 315/405 [02:25<00:34,  2.58it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 316/405 [02:25<00:34,  2.57it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 317/405 [02:25<00:34,  2.57it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 318/405 [02:26<00:33,  2.58it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 319/405 [02:26<00:33,  2.57it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 320/405 [02:27<00:33,  2.55it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 321/405 [02:27<00:33,  2.53it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 322/405 [02:27<00:32,  2.55it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 323/405 [02:28<00:32,  2.56it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 324/405 [02:28<00:31,  2.57it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:703] 2022-12-05 13:44:03,829 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:703] 2022-12-05 13:44:03,829 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2944] 2022-12-05 13:44:03,832 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2944] 2022-12-05 13:44:03,832 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2946] 2022-12-05 13:44:03,832 >>   Num examples = 326\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2949] 2022-12-05 13:44:03,832 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2946] 2022-12-05 13:44:03,832 >>   Num examples = 326\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2949] 2022-12-05 13:44:03,832 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/41 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 3/41 [00:00<00:02, 17.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 5/41 [00:00<00:02, 14.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 7/41 [00:00<00:02, 13.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 9/41 [00:00<00:02, 12.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 11/41 [00:00<00:02, 12.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 13/41 [00:01<00:02, 12.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 15/41 [00:01<00:02, 11.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m41%|████▏     | 17/41 [00:01<00:02, 11.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▋     | 19/41 [00:01<00:01, 11.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 21/41 [00:01<00:01, 11.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 23/41 [00:01<00:01, 11.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 25/41 [00:02<00:01, 11.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 27/41 [00:02<00:01, 11.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 29/41 [00:02<00:01, 11.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 31/41 [00:02<00:00, 11.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 33/41 [00:02<00:00, 11.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 35/41 [00:02<00:00, 11.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 37/41 [00:03<00:00, 11.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 39/41 [00:03<00:00, 11.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 41/41 [00:03<00:00, 11.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m12/05/2022 13:44:07 - INFO - utils_qa - Post-processing 326 example predictions split into 326 features.\u001b[0m\n",
      "\u001b[34m0%|          | 0/326 [00:00<?, ?it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m9%|▊         | 28/326 [00:00<00:01, 275.46it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 56/326 [00:00<00:00, 275.54it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 84/326 [00:00<00:00, 277.29it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 112/326 [00:00<00:00, 273.39it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 140/326 [00:00<00:00, 274.62it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 168/326 [00:00<00:00, 271.08it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 196/326 [00:00<00:00, 268.06it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 224/326 [00:00<00:00, 270.91it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 252/326 [00:00<00:00, 273.54it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 280/326 [00:01<00:00, 263.65it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 308/326 [00:01<00:00, 268.31it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 326/326 [00:01<00:00, 270.82it/s]\u001b[0m\n",
      "\u001b[34m12/05/2022 13:44:09 - INFO - utils_qa - Saving predictions to /opt/ml/model/eval_predictions.json.\u001b[0m\n",
      "\u001b[34m12/05/2022 13:44:09 - INFO - utils_qa - Saving nbest_preds to /opt/ml/model/eval_nbest_predictions.json.\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_exact_match': 93.25153374233129, 'eval_f1': 94.06952965235173, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m80%|████████  | 324/405 [02:33<00:31,  2.57it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 41/41 [00:05<00:00, 11.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                               #033[A\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2693] 2022-12-05 13:44:09,143 >> Saving model checkpoint to /opt/ml/model/checkpoint-324\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2693] 2022-12-05 13:44:09,143 >> Saving model checkpoint to /opt/ml/model/checkpoint-324\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:447] 2022-12-05 13:44:09,144 >> Configuration saved in /opt/ml/model/checkpoint-324/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:447] 2022-12-05 13:44:09,144 >> Configuration saved in /opt/ml/model/checkpoint-324/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1637] 2022-12-05 13:44:09,838 >> Model weights saved in /opt/ml/model/checkpoint-324/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1637] 2022-12-05 13:44:09,838 >> Model weights saved in /opt/ml/model/checkpoint-324/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2157] 2022-12-05 13:44:09,838 >> tokenizer config file saved in /opt/ml/model/checkpoint-324/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2157] 2022-12-05 13:44:09,838 >> tokenizer config file saved in /opt/ml/model/checkpoint-324/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2164] 2022-12-05 13:44:09,839 >> Special tokens file saved in /opt/ml/model/checkpoint-324/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2164] 2022-12-05 13:44:09,839 >> Special tokens file saved in /opt/ml/model/checkpoint-324/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2771] 2022-12-05 13:44:11,321 >> Deleting older checkpoint [/opt/ml/model/checkpoint-243] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2771] 2022-12-05 13:44:11,321 >> Deleting older checkpoint [/opt/ml/model/checkpoint-243] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m80%|████████  | 325/405 [02:36<03:34,  2.69s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 326/405 [02:37<02:37,  1.99s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 327/405 [02:37<01:57,  1.51s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 328/405 [02:37<01:30,  1.17s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 329/405 [02:38<01:11,  1.07it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 330/405 [02:38<00:57,  1.30it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 331/405 [02:38<00:48,  1.53it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 332/405 [02:39<00:41,  1.74it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 333/405 [02:39<00:37,  1.93it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 334/405 [02:40<00:33,  2.09it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 335/405 [02:40<00:31,  2.22it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 336/405 [02:40<00:29,  2.32it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 337/405 [02:41<00:28,  2.40it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 338/405 [02:41<00:27,  2.46it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 339/405 [02:42<00:26,  2.50it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 340/405 [02:42<00:25,  2.52it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 341/405 [02:42<00:25,  2.54it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 342/405 [02:43<00:24,  2.56it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 343/405 [02:43<00:24,  2.57it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 344/405 [02:43<00:23,  2.59it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 345/405 [02:44<00:23,  2.59it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 346/405 [02:44<00:22,  2.59it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 347/405 [02:45<00:22,  2.59it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 348/405 [02:45<00:21,  2.60it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 349/405 [02:45<00:21,  2.60it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 350/405 [02:46<00:21,  2.60it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 351/405 [02:46<00:20,  2.60it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 352/405 [02:47<00:20,  2.61it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 353/405 [02:47<00:19,  2.61it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 354/405 [02:47<00:19,  2.61it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 355/405 [02:48<00:19,  2.61it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 356/405 [02:48<00:18,  2.60it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 357/405 [02:48<00:18,  2.60it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 358/405 [02:49<00:18,  2.61it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 359/405 [02:49<00:17,  2.60it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 360/405 [02:50<00:17,  2.60it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 361/405 [02:50<00:16,  2.60it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 362/405 [02:50<00:16,  2.60it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 363/405 [02:51<00:16,  2.60it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 364/405 [02:51<00:15,  2.60it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 365/405 [02:52<00:15,  2.60it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 366/405 [02:52<00:14,  2.61it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 367/405 [02:52<00:14,  2.60it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 368/405 [02:53<00:14,  2.60it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 369/405 [02:53<00:13,  2.60it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 370/405 [02:53<00:13,  2.60it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 371/405 [02:54<00:13,  2.60it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 372/405 [02:54<00:12,  2.59it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 373/405 [02:55<00:12,  2.58it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 374/405 [02:55<00:11,  2.59it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 375/405 [02:55<00:11,  2.58it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 376/405 [02:56<00:11,  2.59it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 377/405 [02:56<00:10,  2.59it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 378/405 [02:57<00:10,  2.60it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 379/405 [02:57<00:10,  2.59it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 380/405 [02:57<00:09,  2.59it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 381/405 [02:58<00:09,  2.60it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 382/405 [02:58<00:08,  2.60it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 383/405 [02:58<00:08,  2.60it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 384/405 [02:59<00:08,  2.60it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 385/405 [02:59<00:07,  2.59it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 386/405 [03:00<00:07,  2.59it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 387/405 [03:00<00:06,  2.60it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 388/405 [03:00<00:06,  2.58it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 389/405 [03:01<00:06,  2.58it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 390/405 [03:01<00:05,  2.58it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 391/405 [03:02<00:05,  2.57it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 392/405 [03:02<00:05,  2.56it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 393/405 [03:02<00:04,  2.57it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 394/405 [03:03<00:04,  2.56it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 395/405 [03:03<00:03,  2.56it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 396/405 [03:04<00:03,  2.57it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 397/405 [03:04<00:03,  2.58it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 398/405 [03:04<00:02,  2.58it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 399/405 [03:05<00:02,  2.57it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 400/405 [03:05<00:01,  2.58it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 401/405 [03:05<00:01,  2.58it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 402/405 [03:06<00:01,  2.58it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 403/405 [03:06<00:00,  2.59it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 404/405 [03:07<00:00,  2.58it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 405/405 [03:07<00:00,  2.57it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:703] 2022-12-05 13:44:42,726 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:703] 2022-12-05 13:44:42,726 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2944] 2022-12-05 13:44:42,729 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2944] 2022-12-05 13:44:42,729 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2946] 2022-12-05 13:44:42,729 >>   Num examples = 326\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2949] 2022-12-05 13:44:42,729 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2946] 2022-12-05 13:44:42,729 >>   Num examples = 326\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2949] 2022-12-05 13:44:42,729 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/41 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 3/41 [00:00<00:02, 17.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 5/41 [00:00<00:02, 14.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 7/41 [00:00<00:02, 13.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 9/41 [00:00<00:02, 12.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 11/41 [00:00<00:02, 12.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 13/41 [00:01<00:02, 12.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 15/41 [00:01<00:02, 11.97it/s]#033[A\u001b[0m\n",
      "\u001b[34m41%|████▏     | 17/41 [00:01<00:02, 11.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▋     | 19/41 [00:01<00:01, 11.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 21/41 [00:01<00:01, 11.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 23/41 [00:01<00:01, 11.90it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 25/41 [00:02<00:01, 11.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 27/41 [00:02<00:01, 11.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 29/41 [00:02<00:01, 11.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 31/41 [00:02<00:00, 11.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 33/41 [00:02<00:00, 11.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 35/41 [00:02<00:00, 11.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 37/41 [00:03<00:00, 11.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 39/41 [00:03<00:00, 11.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 41/41 [00:03<00:00, 12.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m12/05/2022 13:44:46 - INFO - utils_qa - Post-processing 326 example predictions split into 326 features.\u001b[0m\n",
      "\u001b[34m0%|          | 0/326 [00:00<?, ?it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m9%|▊         | 28/326 [00:00<00:01, 277.52it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 56/326 [00:00<00:00, 278.26it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 85/326 [00:00<00:00, 279.54it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 113/326 [00:00<00:00, 278.55it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 141/326 [00:00<00:00, 278.36it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 169/326 [00:00<00:00, 278.47it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 198/326 [00:00<00:00, 279.02it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 226/326 [00:00<00:00, 278.99it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 254/326 [00:00<00:00, 279.05it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 282/326 [00:01<00:00, 279.24it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 311/326 [00:01<00:00, 279.62it/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 326/326 [00:01<00:00, 278.78it/s]\u001b[0m\n",
      "\u001b[34m12/05/2022 13:44:47 - INFO - utils_qa - Saving predictions to /opt/ml/model/eval_predictions.json.\u001b[0m\n",
      "\u001b[34m12/05/2022 13:44:47 - INFO - utils_qa - Saving nbest_preds to /opt/ml/model/eval_nbest_predictions.json.\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_exact_match': 93.25153374233129, 'eval_f1': 94.06952965235173, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 405/405 [03:12<00:00,  2.57it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 41/41 [00:05<00:00, 12.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                               #033[A\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2693] 2022-12-05 13:44:47,950 >> Saving model checkpoint to /opt/ml/model/checkpoint-405\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2693] 2022-12-05 13:44:47,950 >> Saving model checkpoint to /opt/ml/model/checkpoint-405\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:447] 2022-12-05 13:44:47,951 >> Configuration saved in /opt/ml/model/checkpoint-405/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:447] 2022-12-05 13:44:47,951 >> Configuration saved in /opt/ml/model/checkpoint-405/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1637] 2022-12-05 13:44:48,668 >> Model weights saved in /opt/ml/model/checkpoint-405/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1637] 2022-12-05 13:44:48,668 >> Model weights saved in /opt/ml/model/checkpoint-405/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2157] 2022-12-05 13:44:48,669 >> tokenizer config file saved in /opt/ml/model/checkpoint-405/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2157] 2022-12-05 13:44:48,669 >> tokenizer config file saved in /opt/ml/model/checkpoint-405/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2164] 2022-12-05 13:44:48,669 >> Special tokens file saved in /opt/ml/model/checkpoint-405/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2164] 2022-12-05 13:44:48,669 >> Special tokens file saved in /opt/ml/model/checkpoint-405/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2771] 2022-12-05 13:44:50,136 >> Deleting older checkpoint [/opt/ml/model/checkpoint-324] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2771] 2022-12-05 13:44:50,136 >> Deleting older checkpoint [/opt/ml/model/checkpoint-324] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-12-05 13:44:50,294 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-12-05 13:44:50,294 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2009] 2022-12-05 13:44:50,294 >> Loading best model from /opt/ml/model/checkpoint-81 (score: 93.25153374233129).\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2009] 2022-12-05 13:44:50,294 >> Loading best model from /opt/ml/model/checkpoint-81 (score: 93.25153374233129).\u001b[0m\n",
      "\u001b[34m{'train_runtime': 195.4721, 'train_samples_per_second': 24.863, 'train_steps_per_second': 2.072, 'train_loss': 0.14441242924442999, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 405/405 [03:15<00:00,  2.57it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1919] 2022-12-05 13:44:50,660 >> Deleting older checkpoint [/opt/ml/model/checkpoint-405] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1919] 2022-12-05 13:44:50,660 >> Deleting older checkpoint [/opt/ml/model/checkpoint-405] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m100%|██████████| 405/405 [03:15<00:00,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2693] 2022-12-05 13:44:50,858 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2693] 2022-12-05 13:44:50,858 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:447] 2022-12-05 13:44:50,859 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:447] 2022-12-05 13:44:50,859 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1637] 2022-12-05 13:44:51,567 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1637] 2022-12-05 13:44:51,567 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2157] 2022-12-05 13:44:51,568 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2157] 2022-12-05 13:44:51,568 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2164] 2022-12-05 13:44:51,568 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2164] 2022-12-05 13:44:51,568 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =        5.0\n",
      "  train_loss               =     0.1444\n",
      "  train_runtime            = 0:03:15.47\n",
      "  train_samples            =        972\n",
      "  train_samples_per_second =     24.863\n",
      "  train_steps_per_second   =      2.072\u001b[0m\n",
      "\u001b[34m12/05/2022 13:44:51 - INFO - __main__ - *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:703] 2022-12-05 13:44:51,616 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:703] 2022-12-05 13:44:51,616 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2944] 2022-12-05 13:44:51,619 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2944] 2022-12-05 13:44:51,619 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2946] 2022-12-05 13:44:51,619 >>   Num examples = 326\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2949] 2022-12-05 13:44:51,619 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2946] 2022-12-05 13:44:51,619 >>   Num examples = 326\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2949] 2022-12-05 13:44:51,619 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 3/41 [00:00<00:02, 17.94it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 5/41 [00:00<00:02, 14.28it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 7/41 [00:00<00:02, 13.19it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 9/41 [00:00<00:02, 12.63it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 11/41 [00:00<00:02, 12.31it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 13/41 [00:01<00:02, 12.17it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 15/41 [00:01<00:02, 12.06it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 17/41 [00:01<00:01, 12.00it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 19/41 [00:01<00:01, 11.98it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 21/41 [00:01<00:01, 11.91it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 23/41 [00:01<00:01, 11.90it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 25/41 [00:02<00:01, 11.86it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 27/41 [00:02<00:01, 11.81it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 29/41 [00:02<00:01, 11.81it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 31/41 [00:02<00:00, 11.75it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 33/41 [00:02<00:00, 11.77it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 35/41 [00:02<00:00, 11.79it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 37/41 [00:03<00:00, 11.78it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 39/41 [00:03<00:00, 11.76it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 41/41 [00:03<00:00, 12.21it/s]\u001b[0m\n",
      "\u001b[34m12/05/2022 13:44:55 - INFO - utils_qa - Post-processing 326 example predictions split into 326 features.\u001b[0m\n",
      "\u001b[34m0%|          | 0/326 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 26/326 [00:00<00:01, 256.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 54/326 [00:00<00:01, 265.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 82/326 [00:00<00:00, 269.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▎      | 110/326 [00:00<00:00, 269.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 138/326 [00:00<00:00, 270.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 166/326 [00:00<00:00, 270.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 194/326 [00:00<00:00, 271.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 222/326 [00:00<00:00, 259.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 249/326 [00:00<00:00, 261.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 277/326 [00:01<00:00, 265.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 305/326 [00:01<00:00, 267.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 326/326 [00:01<00:00, 267.05it/s]\u001b[0m\n",
      "\u001b[34m12/05/2022 13:44:56 - INFO - utils_qa - Saving predictions to /opt/ml/model/eval_predictions.json.\u001b[0m\n",
      "\u001b[34m12/05/2022 13:44:56 - INFO - utils_qa - Saving nbest_preds to /opt/ml/model/eval_nbest_predictions.json.\u001b[0m\n",
      "\u001b[34m100%|██████████| 41/41 [00:05<00:00,  7.94it/s]\u001b[0m\n",
      "\u001b[34m***** eval metrics *****\u001b[0m\n",
      "\u001b[34mepoch            =     5.0\n",
      "  eval_exact_match = 93.2515\n",
      "  eval_f1          = 94.0695\n",
      "  eval_samples     =     326\u001b[0m\n",
      "\u001b[34m[INFO|modelcard.py:449] 2022-12-05 13:44:56,905 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\u001b[0m\n",
      "\u001b[34m[INFO|modelcard.py:449] 2022-12-05 13:44:56,905 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\u001b[0m\n",
      "\u001b[34m2022-12-05 13:44:57,742 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-12-05 13:44:57,742 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-12-05 13:44:57,743 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-12-05 13:45:03 Uploading - Uploading generated training model\n",
      "2022-12-05 13:48:29 Completed - Training job completed\n",
      "Training seconds: 719\n",
      "Billable seconds: 719\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters which are passed to the training job\n",
    "hyperparameters = {\n",
    "    # 训练\n",
    "    'model_name_or_path': 'deepset/bert-base-cased-squad2', #  pretrained model name,deepset/roberta-base-squad2,deepset/minilm-uncased-squad2,etc\n",
    "    'train_file': '/opt/ml/input/data/train/train.json',\n",
    "    'validation_file':'/opt/ml/input/data/test/test.json',\n",
    "    'output_dir':'/opt/ml/model',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'per_device_train_batch_size':12,\n",
    "    'learning_rate':3e-5,\n",
    "    'num_train_epochs': 5,\n",
    "    'max_seq_length':512,\n",
    "    'doc_stride':128,\n",
    "    'overwrite_cache': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'evaluation_strategy': 'epoch',\n",
    "    'save_strategy': 'epoch',\n",
    "    'prediction_loss_only': False,\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'eval_exact_match',\n",
    "    'greater_is_better': True,\n",
    "    'save_total_limit': 1\n",
    "}\n",
    "# create the Estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point = 'run_train.py',\n",
    "    source_dir = './code',\n",
    "    instance_type = 'ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    role = role,\n",
    "    framework_version=\"1.11.0\",\n",
    "    transformers_version='4.6',\n",
    "    py_version='py38',\n",
    "    hyperparameters = hyperparameters,\n",
    "    base_job_name='train-QAmodel-stary-4books-eval-on45',\n",
    ")\n",
    "\n",
    "# starting training job\n",
    "estimator.fit({'train':training_input_path,'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8676bb0",
   "metadata": {},
   "source": [
    "### 模型部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6824dd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-551641581032/train-QAmodel-stary-4books-eval-on45-2022-12-05-13-34-55-598/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_data_file = estimator.model_data\n",
    "print(model_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b57c724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-551641581032/train-QAmodel-stary-4books-eval-on45-2022-12-05-13-34-55-598/output/model.tar.gz to model_tmp/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp $model_data_file ./model_tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d310d4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_results.json\n",
      "tokenizer.json\n",
      "eval_nbest_predictions.json\n",
      "special_tokens_map.json\n",
      "checkpoint-81/\n",
      "checkpoint-81/optimizer.pt\n",
      "checkpoint-81/tokenizer.json\n",
      "checkpoint-81/special_tokens_map.json\n",
      "checkpoint-81/config.json\n",
      "checkpoint-81/vocab.txt\n",
      "checkpoint-81/rng_state.pth\n",
      "checkpoint-81/training_args.bin\n",
      "checkpoint-81/scheduler.pt\n",
      "checkpoint-81/tokenizer_config.json\n",
      "checkpoint-81/trainer_state.json\n",
      "checkpoint-81/pytorch_model.bin\n",
      "config.json\n",
      "vocab.txt\n",
      "README.md\n",
      "eval_predictions.json\n",
      "all_results.json\n",
      "training_args.bin\n",
      "tokenizer_config.json\n",
      "trainer_state.json\n",
      "pytorch_model.bin\n",
      "eval_results.json\n"
     ]
    }
   ],
   "source": [
    "!tar -zxvf ./model_tmp/model.tar.gz -C ./model_tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4ceb8ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ./code ./deploy_model/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c1f7b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp model_tmp/config.json ./deploy_model/\n",
    "!cp model_tmp/pytorch_model.bin ./deploy_model/\n",
    "!cp model_tmp/special_tokens_map.json ./deploy_model/\n",
    "!cp model_tmp/tokenizer_config.json ./deploy_model/\n",
    "!cp model_tmp/tokenizer.json ./deploy_model/\n",
    "!cp model_tmp/training_args.bin ./deploy_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "549ee6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/\n",
      "code/requirements.txt\n",
      "code/run_train.py\n",
      "code/trainer_qa.py\n",
      "code/utils_qa.py\n",
      "code/inference.py\n",
      "code/.ipynb_checkpoints/\n",
      "config.json\n",
      "pytorch_model.bin\n",
      "special_tokens_map.json\n",
      "tokenizer_config.json\n",
      "tokenizer.json\n",
      "training_args.bin\n"
     ]
    }
   ],
   "source": [
    "!cd deploy_model && tar -czvf ./model-deploy.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e47bd35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: deploy_model/model-deploy.tar.gz to s3://sagemaker-us-east-1-551641581032/output-stary/model-deploy.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp deploy_model/model-deploy.tar.gz s3://$bucket/output-stary/model-deploy.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "612a9951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!CPU times: user 28.1 s, sys: 3.1 s, total: 31.2 s\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Deploy the model using model_data\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# instance_type = 'local'\n",
    "# instance_type = 'ml.m5.xlarge'\n",
    "instance_type = 'ml.g4dn.xlarge'\n",
    "\n",
    "# predictor = estimator.deploy(initial_instance_count=1, instance_type=instance_type)\n",
    "\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data='s3://{}/output-stary/model-deploy.tar.gz'.format(bucket), role=role,\n",
    "                             entry_point='inference.py', framework_version='1.11.0', py_version='py38', model_server_workers=4)  # TODO [For GPU], model_server_workers=6\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type=instance_type, initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b296aad6",
   "metadata": {},
   "source": [
    "### 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ba09ab8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 ms, sys: 282 µs, total: 10.8 ms\n",
      "Wall time: 253 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Question': 'Who is the speaker?', 'answer': 'Blair', 'Answer_start': 14}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# example request, you always need to define \"inputs\"\n",
    "import time\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "\n",
    "data = {\n",
    "    'inputs': {\n",
    "        \"question\": \"Who is the speaker?\",\n",
    "        \"context\": \"We were halfway through our meal when Blair jumped up and did the potty dance, \\\"I really have to go! I'll meet you guys at the house.\\\" She starts running faster. \"\n",
    "    }\n",
    "}\n",
    "\n",
    "data1 = {\n",
    "    'inputs': {\n",
    "        \"question\": \"Who said the center sentence?\",\n",
    "        \"context\": \"We were halfway through our meal when Blair jumped up and did the potty dance, center: \\\"I really have to go! I'll meet you guys at the house.\\\" She starts running faster. \"\n",
    "    }\n",
    "}\n",
    "\n",
    "# request\n",
    "outputs = predictor.predict(data)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4680b1",
   "metadata": {},
   "source": [
    "### clean-up\n",
    "\n",
    "Deleting the local endpoint when you're finished is important since you can only run one local endpoint at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "935bc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.delete_endpoint()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e0f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
